{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. It was founded in 1985 because of a merger between Houston Natural Gas and InterNorth, both relatively small regional companies. Before its bankruptcy on December 2, 2001, Enron employed approximately 20,000 staff and was one of the world's major electricity, natural gas, communications and pulp and paper companies, with claimed revenues of nearly $101 billion during 2000. Fortune named Enron \"America's Most Innovative Company\" for six consecutive years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, will be built an algorithm to identify Enron Employees who may have committed fraud based on the public Enron financial and email dataset. Identify POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pro-Pc\\Anaconda3\\envs\\py2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot\n",
    "sys.path.append(\"../tools/\")\n",
    "import numpy as np \n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "## Load the dictionary containing the dataset\n",
    "\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains information about 146 persons with 21 features for each person and only 18(12%) POIs form 35 in total. So there is the Class Imbalance Problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'METTS MARK': 21, 'BAXTER JOHN C': 21, 'ELLIOTT STEVEN': 21, 'CORDES WILLIAM R': 21, 'HANNON KEVIN P': 21, 'MORDAUNT KRISTINA M': 21, 'MEYER ROCKFORD G': 21, 'MCMAHON JEFFREY': 21, 'HAEDICKE MARK E': 21, 'PIPER GREGORY F': 21, 'HUMPHREY GENE E': 21, 'NOLES JAMES L': 21, 'BLACHMAN JEREMY M': 21, 'SUNDE MARTIN': 21, 'GIBBS DANA R': 21, 'LOWRY CHARLES P': 21, 'COLWELL WESLEY': 21, 'MULLER MARK S': 21, 'JACKSON CHARLENE R': 21, 'WESTFAHL RICHARD K': 21, 'WALTERS GARETH W': 21, 'WALLS JR ROBERT H': 21, 'KITCHEN LOUISE': 21, 'CHAN RONNIE': 21, 'BELFER ROBERT': 21, 'SHANKMAN JEFFREY A': 21, 'WODRASKA JOHN': 21, 'BERGSIEKER RICHARD P': 21, 'URQUHART JOHN A': 21, 'BIBI PHILIPPE A': 21, 'RIEKER PAULA H': 21, 'WHALEY DAVID A': 21, 'BECK SALLY W': 21, 'HAUG DAVID L': 21, 'ECHOLS JOHN B': 21, 'MENDELSOHN JOHN': 21, 'HICKERSON GARY J': 21, 'CLINE KENNETH W': 21, 'LEWIS RICHARD': 21, 'HAYES ROBERT E': 21, 'KOPPER MICHAEL J': 21, 'LEFF DANIEL P': 21, 'LAVORATO JOHN J': 21, 'BERBERIAN DAVID': 21, 'DETMERING TIMOTHY J': 21, 'WAKEHAM JOHN': 21, 'POWERS WILLIAM': 21, 'GOLD JOSEPH': 21, 'BANNANTINE JAMES M': 21, 'DUNCAN JOHN H': 21, 'SHAPIRO RICHARD S': 21, 'SHERRIFF JOHN R': 21, 'SHELBY REX': 21, 'LEMAISTRE CHARLES': 21, 'DEFFNER JOSEPH M': 21, 'KISHKILL JOSEPH G': 21, 'WHALLEY LAWRENCE G': 21, 'PIRO JIM': 21, 'MCCONNELL MICHAEL S': 21, 'DELAINEY DAVID W': 21, 'SULLIVAN-SHAKLOVITZ COLLEEN': 21, 'WROBEL BRUCE': 21, 'LINDHOLM TOD A': 21, 'MEYER JEROME J': 21, 'LAY KENNETH L': 21, 'BUTTS ROBERT H': 21, 'OLSON CINDY K': 21, 'MCDONALD REBECCA': 21, 'CUMBERLAND MICHAEL S': 21, 'GAHN ROBERT S': 21, 'BADUM JAMES P': 21, 'HERMANN ROBERT J': 21, 'FALLON JAMES B': 21, 'GATHMANN WILLIAM D': 21, 'HORTON STANLEY C': 21, 'BOWEN JR RAYMOND M': 21, 'GILLIS JOHN': 21, 'FITZGERALD JAY L': 21, 'MORAN MICHAEL P': 21, 'REDMOND BRIAN L': 21, 'BAZELIDES PHILIP J': 21, 'BELDEN TIMOTHY N': 21, 'DIMICHELE RICHARD G': 21, 'DURAN WILLIAM D': 21, 'THORN TERENCE H': 21, 'FASTOW ANDREW S': 21, 'FOY JOE': 21, 'CALGER CHRISTOPHER F': 21, 'RICE KENNETH D': 21, 'KAMINSKI WINCENTY J': 21, 'LOCKHART EUGENE E': 21, 'COX DAVID': 21, 'OVERDYKE JR JERE C': 21, 'PEREIRA PAULO V. FERRAZ': 21, 'STABLER FRANK': 21, 'SKILLING JEFFREY K': 21, 'BLAKE JR. NORMAN P': 21, 'SHERRICK JEFFREY B': 21, 'PRENTICE JAMES': 21, 'GRAY RODNEY': 21, 'THE TRAVEL AGENCY IN THE PARK': 21, 'UMANOFF ADAM S': 21, 'KEAN STEVEN J': 21, 'TOTAL': 21, 'FOWLER PEGGY': 21, 'WASAFF GEORGE': 21, 'WHITE JR THOMAS E': 21, 'CHRISTODOULOU DIOMEDES': 21, 'ALLEN PHILLIP K': 21, 'SHARP VICTORIA T': 21, 'JAEDICKE ROBERT': 21, 'WINOKUR JR. HERBERT S': 21, 'BROWN MICHAEL': 21, 'MCCLELLAN GEORGE': 21, 'HUGHES JAMES A': 21, 'REYNOLDS LAWRENCE': 21, 'PICKERING MARK R': 21, 'BHATNAGAR SANJAY': 21, 'CARTER REBECCA C': 21, 'BUCHANAN HAROLD G': 21, 'YEAP SOON': 21, 'MURRAY JULIA H': 21, 'GARLAND C KEVIN': 21, 'DODSON KEITH': 21, 'YEAGER F SCOTT': 21, 'HIRKO JOSEPH': 21, 'DIETRICH JANET R': 21, 'DERRICK JR. JAMES V': 21, 'FREVERT MARK A': 21, 'PAI LOU L': 21, 'HAYSLETT RODERICK J': 21, 'BAY FRANKLIN R': 21, 'MCCARTY DANNY J': 21, 'FUGH JOHN L': 21, 'SCRIMSHAW MATTHEW': 21, 'KOENIG MARK E': 21, 'SAVAGE FRANK': 21, 'IZZO LAWRENCE L': 21, 'TILNEY ELIZABETH A': 21, 'MARTIN AMANDA K': 21, 'BUY RICHARD B': 21, 'GRAMM WENDY L': 21, 'CAUSEY RICHARD A': 21, 'TAYLOR MITCHELL S': 21, 'DONAHUE JR JEFFREY M': 21, 'GLISAN JR BEN F': 21}\n"
     ]
    }
   ],
   "source": [
    "# number of fratures for each person\n",
    "\n",
    "features = {}\n",
    "\n",
    "for key, value in data_dict.iteritems():\n",
    "    features[key] = len(value)\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "#number of POIs in dataset\n",
    "\n",
    "count = 0\n",
    "for key in data_dict:\n",
    "    if data_dict[key]['poi']==True:\n",
    "        count+=1\n",
    "print count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Pre-selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For begining, is needed to create feature list and clean the data from outlayers and NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'salary': 365788, 'to_messages': 807, 'deferral_payments': 'NaN', 'total_payments': 1061827, 'exercised_stock_options': 'NaN', 'bonus': 600000, 'restricted_stock': 585062, 'shared_receipt_with_poi': 702, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 585062, 'expenses': 94299, 'loan_advances': 'NaN', 'from_messages': 29, 'other': 1740, 'from_this_person_to_poi': 1, 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'mark.metts@enron.com', 'from_poi_to_this_person': 38}\n"
     ]
    }
   ],
   "source": [
    "#print all features for one person to create feature list\n",
    "\n",
    "print data_dict['METTS MARK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create full feature list\n",
    "\n",
    "features_list = ['poi','bonus','salary','total_payments','deferral_payments','deferred_income',\n",
    "                 'director_fees','expenses','loan_advances','long_term_incentive','other',\n",
    "                 'restricted_stock','restricted_stock_deferred','exercised_stock_options','total_stock_value',\n",
    "                 'to_messages','from_messages','from_poi_to_this_person','from_this_person_to_poi','shared_receipt_with_poi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From plot there is observed an outlier which corresponeds to \"TOTAL\" and for all features we need to remove this value. Also THE TRAVEL AGENCY IN THE PARK does not seem to be a person. In featureFormat function remove_NaN=True replaces NaNs with 0. and remove_all_zeroes=True will omit any data points for which\n",
    "all the features you seek are 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2wQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8da7a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NaN values will be replaced with 0 and features with all 0 values will be removed\n",
    "\n",
    "data = featureFormat(data_dict, features_list, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = True)\n",
    "\n",
    "for point in data:\n",
    "    salary = point[2]\n",
    "    bonus = point[1]\n",
    "    matplotlib.pyplot.scatter( salary, bonus )\n",
    "\n",
    "matplotlib.pyplot.xlabel(\"salary\")\n",
    "matplotlib.pyplot.ylabel(\"bonus\")\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there was one person with all NaN values as from 146 person left 145."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxKNQEBERj0JBREQ8CgUREfH8f7L28XtDSDAeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9441c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove outliers\n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK',0)\n",
    "data_dict.pop('LOCKHART EUGENE E',0)\n",
    "\n",
    "data = featureFormat(data_dict, features_list, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = True)\n",
    "#plot again for check\n",
    "\n",
    "for point in data:\n",
    "    salary = point[2]\n",
    "    bonus = point[1]\n",
    "    matplotlib.pyplot.scatter( salary, bonus )\n",
    "\n",
    "matplotlib.pyplot.xlabel(\"salary\")\n",
    "matplotlib.pyplot.ylabel(\"bonus\")\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total_payments', 'loan_advances')\n",
      "0.964305357071\n",
      "\n",
      "\n",
      "('total_payments', 'other')\n",
      "0.830105039798\n",
      "\n",
      "\n",
      "('loan_advances', 'total_payments')\n",
      "0.964305357071\n",
      "\n",
      "\n",
      "('other', 'total_payments')\n",
      "0.830105039798\n",
      "\n",
      "\n",
      "('exercised_stock_options', 'total_stock_value')\n",
      "0.96387437986\n",
      "\n",
      "\n",
      "('total_stock_value', 'exercised_stock_options')\n",
      "0.96387437986\n",
      "\n",
      "\n",
      "('to_messages', 'shared_receipt_with_poi')\n",
      "0.881262542007\n",
      "\n",
      "\n",
      "('shared_receipt_with_poi', 'to_messages')\n",
      "0.881262542007\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check features for correlation\n",
    "return_list = [] \n",
    "for feature in range(0,20):\n",
    "    tmp_list = []\n",
    "    for key in data:\n",
    "        value = key[feature]\n",
    "        tmp_list.append( float(value) )\n",
    "    return_list.append( np.array(tmp_list) )   \n",
    "\n",
    "#cor,pval  = stats.pearsonr(return_list[15],return_list[19])    \n",
    "    \n",
    "import scipy.stats as stats\n",
    "for i in range(0,20):\n",
    "    for j in range(1,20):\n",
    "        cor,pval  = stats.pearsonr(return_list[i],return_list[j])\n",
    "        if 1>cor>abs(0.8):\n",
    "            print (features_list[i], features_list[j] )\n",
    "            print cor\n",
    "            print \"\\n\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature fraction_from_poi and fraction_to_poi defined as number messages to/from POI (numerator) and number of all messages to/from a person (denominator). The reason of creating these features is that POIs may be exchanging emails between each other more frequently than other people. Its interesting to know what percentage of total emails send/revived to/from POI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeFraction( poi_messages, all_messages ):\n",
    "\n",
    "    fraction = 0.\n",
    "    \n",
    "    if poi_messages!=\"NaN\":\n",
    "        if all_messages!=\"NaN\":\n",
    "            fraction = (poi_messages*1.0)/(all_messages*1.0)\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    return fraction\n",
    "\n",
    "\n",
    "submit_dict = {}\n",
    "for name in data_dict:\n",
    "\n",
    "    data_point = data_dict[name]\n",
    "\n",
    "    \n",
    "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
    "    to_messages = data_point[\"to_messages\"]\n",
    "    fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n",
    "    data_point[\"fraction_from_poi\"] = fraction_from_poi\n",
    "\n",
    "\n",
    "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
    "    from_messages = data_point[\"from_messages\"]\n",
    "    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n",
    "    submit_dict[name]={\"from_poi_to_this_person\":fraction_from_poi,\n",
    "                       \"from_this_person_to_poi\":fraction_to_poi}\n",
    "    data_point[\"fraction_to_poi\"] = fraction_to_poi\n",
    "    \n",
    "    \n",
    "#####################\n",
    "\n",
    "my_data = {}\n",
    "my_data.update(submit_dict)  # Modifies my_data, not data\n",
    "my_data.update(data_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "\n",
    "for key, value in my_data.iteritems():\n",
    "    features[key] = len(value)\n",
    "print (len(features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at all features original full list plus newly created. Check for correlation and remove ones with correlation higher than 80%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total_payments', 'loan_advances')\n",
      "0.964305357071\n",
      "\n",
      "\n",
      "('total_payments', 'other')\n",
      "0.830105039798\n",
      "\n",
      "\n",
      "('loan_advances', 'total_payments')\n",
      "0.964305357071\n",
      "\n",
      "\n",
      "('other', 'total_payments')\n",
      "0.830105039798\n",
      "\n",
      "\n",
      "('exercised_stock_options', 'total_stock_value')\n",
      "0.96387437986\n",
      "\n",
      "\n",
      "('total_stock_value', 'exercised_stock_options')\n",
      "0.96387437986\n",
      "\n",
      "\n",
      "('to_messages', 'shared_receipt_with_poi')\n",
      "0.881262542007\n",
      "\n",
      "\n",
      "('shared_receipt_with_poi', 'to_messages')\n",
      "0.881262542007\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#update features list \n",
    "\n",
    "features_list = ['poi','bonus','salary','total_payments','deferral_payments','deferred_income',\n",
    "                 'director_fees','expenses','loan_advances','long_term_incentive','other',\n",
    "                 'restricted_stock','restricted_stock_deferred','exercised_stock_options','total_stock_value',\n",
    "                 'to_messages','from_messages','from_poi_to_this_person','from_this_person_to_poi','shared_receipt_with_poi','fraction_from_poi','fraction_to_poi']\n",
    "\n",
    "data = featureFormat(my_data, features_list, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False)\n",
    "\n",
    "   \n",
    "return_list = [] \n",
    "for feature in range(0,22):\n",
    "    tmp_list = []\n",
    "    for key in data:\n",
    "        value = key[feature]\n",
    "        tmp_list.append( float(value) )\n",
    "    return_list.append( np.array(tmp_list) )   \n",
    "\n",
    "    \n",
    "import scipy.stats as stats\n",
    "for i in range(0,22):\n",
    "    for j in range(0,22):\n",
    "        cor,pval  = stats.pearsonr(return_list[i],return_list[j])\n",
    "        if 1>cor>abs(0.8):\n",
    "            print (features_list[i], features_list[j] )\n",
    "            print cor\n",
    "            print \"\\n\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create two sets of feature list: one including user created variables and one without these variables.\n",
    "In feature list without user created (original) variables:\n",
    "Can be removed 'shared_receipt_with_poi','exercised_stock_options', 'loan_advances', 'other' because of high correlation.\n",
    "In feature list with user created variables:\n",
    "Can be removed 'from_poi_to_this_person','from_this_person_to_poi','to_messages','from_messages' as informaion from these variables are in newly created features, also 'exercised_stock_options', 'loan_advances', 'other' because of high correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selected features list without user created variables\n",
    "features_list_orig = ['poi','bonus','salary','total_payments','deferral_payments','deferred_income',\n",
    "                 'director_fees','expenses','long_term_incentive',\n",
    "                 'restricted_stock','restricted_stock_deferred','total_stock_value',\n",
    "                 'to_messages','from_messages','from_poi_to_this_person','from_this_person_to_poi']\n",
    "\n",
    "data_orig = featureFormat(data_dict, features_list, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False)\n",
    "labels_orig, features_orig = targetFeatureSplit(data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selected features list\n",
    "\n",
    "features_list = ['poi','bonus','salary','total_payments','deferral_payments','deferred_income',\n",
    "                 'expenses','long_term_incentive',\n",
    "                 'restricted_stock','restricted_stock_deferred','total_stock_value',\n",
    "                 'shared_receipt_with_poi','fraction_from_poi','fraction_to_poi','director_fees']\n",
    "\n",
    "data = featureFormat(my_data, features_list, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-selected feature list is given to poi_id.py and built variety of classifyers to select the one which gives best f1 score. Tried GaussianNB, SVM, DecisionTreeClassifier and RandomForestm and used grid search to get best tuned parameters in order to get the best perfomance for each of them. There is created function which returns the best perfomance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Feature selection\n",
    "\n",
    "**SelectKBest**\n",
    "Select features according to the k highest scores. Parameters are **score_func:** Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores in this case is used ch2 and **k:** Number of top features to select. In this case is tested different number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA()\n",
    "\n",
    "# KBest\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "skb = SelectKBest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Feature scaling\n",
    "**MinMaxScaler** is used for scaling features. It transforms features by scaling each feature to a given range.\n",
    "This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one. The main parameter is **feature_range** : tuple (min, max), default=(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features=scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Classification algorithms\n",
    "\n",
    "**Gaussian Naive Bayes (GaussianNB)**\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features.For GaussianNB the likelihood of the features is assumed to be Gaussian.\n",
    "\n",
    "**Support vector machines (SVMs)**\n",
    "\n",
    "The SVMs are a set of supervised learning methods used for classification, regression and outliers detection. Used C-Support Vector Classification. **Parameters used:** C : Penalty parameter C of the error term. **kernel :**  ‘poly’ and ‘rbf’. \n",
    "\n",
    "**Decision Trees (DTs)**\n",
    "\n",
    "DTs are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. **Parameters used:** **criterion :** The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. **min_samples_split :** The minimum number of samples required to split an internal node. **max_depth :** The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "**Ensemble methods**\n",
    "\n",
    "The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced. By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble. In this case is used ** Forests of randomized trees** and **AdaBoost**. \n",
    "\n",
    "** Forests of randomized trees**\n",
    "The **RandomForest** algorithm is perturb-and-combine techniques specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. **Parameters used:** **n_estimators :** The number of trees in the forest. **criterion** , **min_samples_split**,  **max_depth ** (for definitions see **Decision Trees**).\n",
    "\n",
    "**AdaBoost**\n",
    "The core principle of **AdaBoost** is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. **Parameters used:** **\n",
    "n_estimators :** The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n",
    "\n",
    ">**Parameters used:** Means that various values is tested for these parameters, rest use default settings in the other word these are parameters which are tuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NaiveBayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_clf = GaussianNB()\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC()\n",
    "\n",
    "# DecisionTree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "# RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ab_clf = AdaBoostClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Best Algorithm Selection and Parameters Tuning \n",
    "\n",
    "Different model training algorithms require different hyperparameters, some simple algorithms require none. Hyperparameters are parameters whose values are set before the learning process begins and their optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. Usually a metric is chosen to measure the algorithm's performance on an independent data set and hyperparameters that maximize this measure are adopted. The cross-validation is process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test the generalization ability of a trained model and is used to estimate this generalization performance. \n",
    "**GridSearchCV** is used for exhaustive search over specified parameter values for an estimator. GridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used. The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid. \n",
    "**Pipeline** of transforms with a final estimator is also used in **GridSearch**. It sequentially applies a list of transforms and a final estimator. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The define which is the best from all five algorithm with best tuned parameters from grid search is compared one by one to clearly see the deferences between the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit, train_test_split, cross_val_score\n",
    "\n",
    "# With user created variables\n",
    "sk_fold = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
    "\n",
    "# Without user created variables\n",
    "sk_fold_orig = StratifiedShuffleSplit(labels_orig, 100, random_state = 42)\n",
    "\n",
    "# Having handle to validation scores\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score)\n",
    "\n",
    "# pipelines and grid serch\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 9 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pro-Pc\\Anaconda3\\envs\\py2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=12, score_func=<function f_classif at 0x000000000A26D7B8>)), ('nb', GaussianNB(priors=None))])\n",
      "('best_score_ :', 0.36109701409701417)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 900 out of 900 | elapsed:    4.7s finished\n"
     ]
    }
   ],
   "source": [
    "pipeline_nb = Pipeline(steps=[('scaling',scaler),('reduce_dim', skb), (\"nb\", nb_clf)])\n",
    "    \n",
    "parameters_nb = {\n",
    "'reduce_dim__k': range(5,14),\n",
    "    }\n",
    "\n",
    "gs_nb = GridSearchCV(pipeline_nb, parameters_nb, verbose=1, cv=sk_fold, scoring = 'f1')\n",
    "gs_nb.fit((features), (labels))\n",
    "        \n",
    "print gs_nb.best_estimator_ \n",
    "\n",
    "print (\"best_score_ :\", gs_nb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 72 candidates, totalling 7200 fits\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=13, score_func=<function f_classif at 0x000000000A26D7B8>)), ('svm', SVC(C=10000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "('best_score_ :', 0.23785714285714285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 7200 out of 7200 | elapsed:  1.0min finished\n"
     ]
    }
   ],
   "source": [
    "pipeline_svm = Pipeline(steps=[('scaling',scaler),('reduce_dim', skb),('svm', svm_clf)])\n",
    "    \n",
    "parameters_svm = {\n",
    "    'svm__C': [1., 10, 100, 10000],\n",
    "    'svm__kernel': ['rbf', 'poly'],\n",
    "    'reduce_dim__k': range(5,14),\n",
    "    }\n",
    "\n",
    "gs_svm = GridSearchCV(pipeline_svm, parameters_svm, verbose=1, cv=sk_fold, scoring = 'f1')\n",
    "gs_svm.fit((features), (labels))\n",
    "        \n",
    "print gs_svm.best_estimator_ \n",
    "\n",
    "print (\"best_score_ :\", gs_svm.best_score_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 360 candidates, totalling 36000 fits\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=11, score_func=<function f_classif at 0x000000000A26D7B8>)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "        ...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "('best_score_ :', 0.39166666666666666)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 36000 out of 36000 | elapsed:  3.9min finished\n"
     ]
    }
   ],
   "source": [
    "#With user created variables\n",
    "pipeline_dt = Pipeline(steps=[('scaling',scaler),('reduce_dim', skb), ('dt', dt_clf)])\n",
    "    \n",
    "parameters_dt = {\n",
    "    'dt__criterion': ['gini', 'entropy'],\n",
    "    'dt__min_samples_split': [2, 3, 4, 5],\n",
    "    'dt__max_depth': [None, 2, 5, 7, 10],\n",
    "    'reduce_dim__k': range(5,14),\n",
    "    }\n",
    "\n",
    "gs_dt = GridSearchCV(pipeline_dt, parameters_dt, verbose=1, cv=sk_fold, scoring = 'f1')\n",
    "gs_dt.fit((features), (labels))\n",
    "        \n",
    "print gs_dt.best_estimator_ \n",
    "\n",
    "print (\"best_score_ :\", gs_dt.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 440 candidates, totalling 44000 fits\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=15, score_func=<function f_classif at 0x000000000A26D7B8>)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "        ...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "('best_score_ :', 0.40000000000000002)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 44000 out of 44000 | elapsed:  4.9min finished\n"
     ]
    }
   ],
   "source": [
    "#Without user created variables\n",
    "pipeline_dt = Pipeline(steps=[('scaling',scaler),('reduce_dim', skb), ('dt', dt_clf)])\n",
    "    \n",
    "parameters_dt = {\n",
    "    'dt__criterion': ['gini', 'entropy'],\n",
    "    'dt__min_samples_split': [2, 3, 4, 5],\n",
    "    'dt__max_depth': [None, 2, 5, 7, 10],\n",
    "    'reduce_dim__k': range(5,16),\n",
    "    }\n",
    "\n",
    "gs_dt_orig = GridSearchCV(pipeline_dt, parameters_dt, verbose=1, cv=sk_fold_orig, scoring = 'f1')\n",
    "gs_dt_orig.fit((features_orig), (labels_orig))\n",
    "        \n",
    "print gs_dt_orig.best_estimator_ \n",
    "\n",
    "print (\"best_score_ :\", gs_dt_orig.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 864 candidates, totalling 86400 fits\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=13, score_func=<function f_classif at 0x000000000A26D7B8>)), ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "('best_score_ :', 0.31338095238095237)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 86400 out of 86400 | elapsed: 43.1min finished\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf = Pipeline(steps=[('scaling',scaler),('reduce_dim', skb), ('rf', rf_clf)])\n",
    "    \n",
    "parameters_rf = {\n",
    "    'rf__n_estimators': [5, 7, 10, 14],\n",
    "    'rf__criterion': ['gini', 'entropy'],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__max_depth': [None, 2, 5, 10],    \n",
    "    'reduce_dim__k': range(5,14),\n",
    "    }\n",
    "\n",
    "gs_rf = GridSearchCV(pipeline_rf, parameters_rf, verbose=1, cv=sk_fold, scoring = 'f1')\n",
    "gs_rf.fit((features), (labels))\n",
    "        \n",
    "print gs_rf.best_estimator_ \n",
    "\n",
    "print (\"best_score_ :\", gs_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 27 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2700 out of 2700 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=10, score_func=<function f_classif at 0x000000000A26D7B8>)), ('ab', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=100, random_state=None))])\n",
      "('best_score_ :', 0.315)\n"
     ]
    }
   ],
   "source": [
    "pipeline_ada = Pipeline(steps=[('scaling',scaler),('reduce_dim', skb), ('ab', ab_clf)])\n",
    "    \n",
    "parameters_ada = {\n",
    "    'ab__n_estimators': [25, 50, 100],\n",
    "    'reduce_dim__k': range(5,14),\n",
    "    }\n",
    "    \n",
    "gs_ada = GridSearchCV(pipeline_ada, parameters_ada, verbose=1, cv=sk_fold, scoring = 'f1')\n",
    "gs_ada.fit((features), (labels))\n",
    "        \n",
    "print gs_ada.best_estimator_ \n",
    "\n",
    "print (\"best_score_ :\", gs_ada.best_score_)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  cross-validation\n",
    "\n",
    "Stratified **ShuffleSplit** cross-validator is used, which provides train/test indices to split data in train/test sets.\n",
    "This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class. Stratification is the process of rearranging the data as to ensure each fold is a good representative of the whole. **Parameters used:** **y**-labels of samples.\n",
    "**n_iter** -Number of re-shuffling and splitting iterations.  **random_state**-If int, random_state is the seed used by the random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below split the sample to train/test sets and checks performance of classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, data, feature_list, folds = 1000):\n",
    "    #data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)  \n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in sk_fold: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        \n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1              \n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "                \n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check perfomance of  NaiveBayes, DecisionTree and AdaBoos, as ther best scores are close to each other. (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=12, score_func=<function f_classif at 0x000000000A26D7B8>)), ('nb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.56067\tPrecision: 0.21491\tRecall: 0.86500\tF1: 0.34428\tF2: 0.53894\n",
      "\tTotal predictions: 1500\tTrue positives:  173\tFalse positives:  632\tFalse negatives:   27\tTrue negatives:  668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(gs_nb.best_estimator_  , data, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below tunde parameters with original feature set is different but F1 score is higher with the set including user selected variables. As final set of features the one using user created variables is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=11, score_func=<function f_classif at 0x000000000A26D7B8>)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "        ...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\tAccuracy: 0.83800\tPrecision: 0.41296\tRecall: 0.51000\tF1: 0.45638\tF2: 0.48711\n",
      "\tTotal predictions: 1500\tTrue positives:  102\tFalse positives:  145\tFalse negatives:   98\tTrue negatives: 1155\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With user created featue set\n",
    "test_classifier(gs_dt.best_estimator_  , data, features_list, folds = 1000)\n",
    "gs_dt.best_estimator_.steps[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=15, score_func=<function f_classif at 0x000000000A26D7B8>)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "        ...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\tAccuracy: 0.84733\tPrecision: 0.42328\tRecall: 0.40000\tF1: 0.41131\tF2: 0.40445\n",
      "\tTotal predictions: 1500\tTrue positives:   80\tFalse positives:  109\tFalse negatives:  120\tTrue negatives: 1191\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=3,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original featue set\n",
    "test_classifier(gs_dt_orig.best_estimator_  , data_orig, features_list_orig, folds = 1000)\n",
    "gs_dt_orig.best_estimator_.steps[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=10, score_func=<function f_classif at 0x000000000A26D7B8>)), ('ab', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=100, random_state=None))])\n",
      "\tAccuracy: 0.85200\tPrecision: 0.42143\tRecall: 0.29500\tF1: 0.34706\tF2: 0.31383\n",
      "\tTotal predictions: 1500\tTrue positives:   59\tFalse positives:   81\tFalse negatives:  141\tTrue negatives: 1219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(gs_ada.best_estimator_  , data, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result the best perfomance is given by DecisionTreeClassifier with 12 features. Presented list of final selected features with their importance k best and DecisionTree. The created variable fraction_from_poi is in the final feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('poi', 3.2001180371896341e-05)\n",
      "('bonus', 3.1275207924891641e-05)\n",
      "('salary', 0.0034921520690395277)\n",
      "('deferral_payments', 0.68377293511464798)\n",
      "('deferred_income', 0.010085033773645459)\n",
      "('expenses', 0.057702561238666773)\n",
      "('long_term_incentive', 0.0076260532052459173)\n",
      "('restricted_stock_deferred', 0.0028746715404537539)\n",
      "('total_stock_value', 0.75002695370502681)\n",
      "('shared_receipt_with_poi', 3.0718610875263968e-06)\n",
      "('fraction_from_poi', 0.0079445345883141817)\n"
     ]
    }
   ],
   "source": [
    "##k best\n",
    "\n",
    "mask=gs_dt.best_estimator_.steps[1][1].get_support()\n",
    "new_features = [] # The list of your K best features\n",
    "\n",
    "for bool, feature in zip(mask, features_list):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "\n",
    "#print selected features and their scores\n",
    "for i in range(0,len(new_features)):        \n",
    "    print (new_features[i], gs_dt.best_estimator_.named_steps['reduce_dim'].pvalues_[i] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the best perfomance shown by DecisionTree with k=10 variables, containing created.\n",
    "Was tested variables in range 5 to 15, i.e at least 5 features to full feature list. \n",
    "\n",
    "The best 10 variable list with their scores:\n",
    "\n",
    "('poi', 11.129479151071294)\n",
    "('bonus', 11.196268305382173)\n",
    "('salary', 2.7678986261219913)\n",
    "('deferral_payments', 5.3040593155079989)\n",
    "('deferred_income', 5.9062230784659526)\n",
    "('expenses', 2.611274332028505)\n",
    "('long_term_incentive', 6.5769490767620624)\n",
    "('restricted_stock_deferred', 14.691308652557398)\n",
    "('total_stock_value', 5.495321379933408)\n",
    "('fraction_from_poi', 8.2430164382598381)\n",
    "\n",
    "\n",
    "**Accuracy: 0.84   Precision: 0.40   Recall: 0.49   F1: 0.44   F2: 0.47**\n",
    "\n",
    "Precision: 0.40 means that only in 40% cases our prediction is truly POI and the rest 60% is the false alarm. Recall: is a ratio of  truely predicted positive and real number of positives  in the other words our prediction is correctly identifying POIs only in 49% cases. F1 score is a measure of a test's accuracy and it is 44% in this case. According these numbers identifing of POIs is not possible with good pressision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('poi', 11.129479151071294)\n",
      "('bonus', 11.196268305382173)\n",
      "('salary', 2.7678986261219913)\n",
      "('deferral_payments', 5.3040593155079989)\n",
      "('deferred_income', 5.9062230784659526)\n",
      "('expenses', 2.611274332028505)\n",
      "('long_term_incentive', 6.5769490767620624)\n",
      "('restricted_stock_deferred', 14.691308652557398)\n",
      "('total_stock_value', 5.495321379933408)\n",
      "('fraction_from_poi', 8.2430164382598381)\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', SelectKBest(k=10, score_func=<function f_classif at 0x000000000A26D7B8>)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "        ...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\tAccuracy: 0.83480\tPrecision: 0.40285\tRecall: 0.49550\tF1: 0.44439\tF2: 0.47371\n",
      "\tTotal predictions: 15000\tTrue positives:  991\tFalse positives: 1469\tFalse negatives: 1009\tTrue negatives: 11531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run tester.py"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
